A normal ML mein pattern identify hota hai within the given data
classification problem ki mail spam hai ya ni......ya recommendation system Jahan

1. A normal chatbot which is built using LLM - doesn't have memory and is not autonomous
2. The chatbot improved with RAG - means the chatbot will provide answer from the shared set of files...but again no memory and not autonomous
3. Agentic AI Chatbot - give the goal ...it will take initiatives and ask for approvals...if it is integrated with certain tools(api) it can automate quite few things
 --> Generative AI is a building block of Agentic AI


#################
LANGCHAIN

-> open source framework to build LLM based apps 
-> similar frameworks are haystacks and llamaindex
-> so initially if we use gemini and once created we need to change the model to openai or change the cloud service from aws to gcp so very minimal 
code changes are required
-> using langchains u can interact with language models (LLM - text input ...output is also text) and embedding models(text as input...vector as output...
used mostly for semantic search Jahan pe embeddings rehte means vectors)
-> LLM API calls are stateless


-> 6 diff components - models,prompts,chains,memory,indexes,agents

-> models: core interfaces through which you interact with AI Models.....sabhi LLm models ke API ko use karne ka tareeka diff hai toh yeh langchain ka interface hai Jahan se har ek AI model se standard way se interaction possible hai

models are of 2 types : language and embedding
language models are of 2 types: LLM and chat models
LLM : general purpose models that is used for raw text generation. They take a string or plain text as input and return a string.
Chat Models: specialised for conversational tasks. They take sequence of messages as inputs and return chat messages as output

-> chains : to build pipeline...automatically gets the output from step 1 and gives it to step 2
parallel chain ( input ko parallel alag alag LLm mein dena) , conditional chains (based on conditions LLM usko execute karega)

-> indexes: connect ur apps to external knowledge - such as PDF,website or DB
these 4 things make up the indexes - doc loader, text splitter, vector store, retrievers

Pehle load kiya...unko chunks mein divide kiya...embeddings mein convert kiya ...then storing the vectors...then retrieving it as per the user query...
toh user query bhi embeddings mein convert hota hai...Jahan pe vectors match hote hai who retrieve hota hai

-> memory
ConversationBufferMemory : stores a transcript of recent mssgs. Great for short chats but can grow largely
ConversationBufferWindowMemory: only keeps the last N interactions to avoid excessive token usage
SummariserBasedMemory: periodically summarises older chat segment to keep a condensed memory footprint
CustomMemory: for advanced use cases, you can store specialised state


==> langchain mein LLM use karke
from langchain_openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
llm = OpenAI(model='gpt-3.5-turbo-instruct')
result = llm.invoke("What is the capital of India")
print(result)


==> langchain mein ChatModels use karke

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()
model = ChatOpenAI(model='gpt-4', temperature=1.5, max_completion_tokens=10)
result = model.invoke("Write a 5 line poem on cricket")
print(result.content)

from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
load_dotenv()
model = ChatGoogleGenerativeAI(model='gemini-1.5-pro')
result = model.invoke('What is the capital of India')
print(result.content)

-> HuggingFace mein API use karke open source models call karte hue
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv
load_dotenv()
llm = HuggingFaceEndpoint(
    repo_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    task="text-generation"
)
model = ChatHuggingFace(llm=llm)
result = model.invoke("What is the capital of India")
print(result.content)

-> HuggingFace se model download karke use karna ho toh
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
import os
os.environ['HF_HOME'] = 'D:/huggingface_cache'
llm = HuggingFacePipeline.from_model_id(
    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',
    task='text-generation',
    pipeline_kwargs=dict(
        temperature=0.5,
        max_new_tokens=100
    )
)
model = ChatHuggingFace(llm=llm
result = model.invoke("What is the capital of India")
print(result.content)


NOTE : .env file mein joh variables use karna hai for API KEY woh fixed hai
OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY, HUGGINGFACEHUB_ACCESS_TOKEN
tabhi hi jabh hum from dotenv import load_dotenv karenge toh api keys dhang se load honge


==> Embedding Models
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
load_dotenv()
embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)
result = embedding.embed_query("Delhi is the capital of India")
print(str(result))



from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()
embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)
documents = [
    "Delhi is the capital of India",
    "Kolkata is the capital of West Bengal",
    "Paris is the capital of France"
]
result = embedding.embed_documents(documents)
print(str(result))

######
NOTE
Temperature ki value 0-2 ke beech hoti hai...0 matlab same output for same input ...2 means utna hi zyada random and diff output aayega


######################
PROMPTS
There are 2 types of prompts which we can give - text based and multimodel like image or video

PromptTemplate - is the langchain library function which will help us write dynamic prompts

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import streamlit as st
from langchain_core.prompts import PromptTemplate

load_dotenv()
model = ChatOpenAI()

st.header('Reasearch Tool')

paper_input = st.selectbox( "Select Research Paper Name", ["Attention Is All You Need", "BERT: Pre-training of Deep Bidirectional Transformers", "GPT-3: Language Models are Few-Shot Learners", "Diffusion Models Beat GANs on Image Synthesis"] )

style_input = st.selectbox( "Select Explanation Style", ["Beginner-Friendly", "Technical", "Code-Oriented", "Mathematical"] ) 

length_input = st.selectbox( "Select Explanation Length", ["Short (1-2 paragraphs)", "Medium (3-5 paragraphs)", "Long (detailed explanation)"] )

template = load_prompt('template.json')



if st.button('Summarize'):
    chain = template | model
    result = chain.invoke({
        'paper_input':paper_input,
        'style_input':style_input,
        'length_input':length_input
    })
    st.write(result.content)


-> writing such prompts wil make the code look bulky so we save these templetes in a new file
let the file name be - prompt_generator.py

from langchain_core.prompts import PromptTemplate

# template
template = PromptTemplate(
    template="""
Please summarize the research paper titled "{paper_input}" with the following specifications:
Explanation Style: {style_input}  
Explanation Length: {length_input}  
1. Mathematical Details:  
   - Include relevant mathematical equations if present in the paper.  
   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  
2. Analogies:  
   - Use relatable analogies to simplify complex ideas.  
If certain information is not available in the paper, respond with: "Insufficient information available" instead of guessing.  
Ensure the summary is clear, accurate, and aligned with the provided style and length.
""",
input_variables=['paper_input', 'style_input','length_input'],
validate_template=True
)

template.save('template.json')

-> template.json file mein humne saara template store kar liya

{
    "name": null,
    "input_variables": [
        "length_input",
        "paper_input",
        "style_input"
    ],
    "optional_variables": [],
    "output_parser": null,
    "partial_variables": {},
    "metadata": null,
    "tags": null,
    "template": "\nPlease summarize the research paper titled \"{paper_input}\" with the following specifications:\nExplanation Style: {style_input}  \nExplanation Length: {length_input}  \n1. Mathematical Details:  \n   - Include relevant mathematical equations if present in the paper.  \n   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \n2. Analogies:  \n   - Use relatable analogies to simplify complex ideas.  \nIf certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.  \nEnsure the summary is clear, accurate, and aligned with the provided style and length.\n",
    "template_format": "f-string",
    "validate_template": true,
    "_type": "prompt"
}


-> now i may load it in my .py file and include load_prompt
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import streamlit as st
from langchain_core.prompts import PromptTemplate,load_prompt

load_dotenv()
model = ChatOpenAI()

st.header('Reasearch Tool')

paper_input = st.selectbox( "Select Research Paper Name", ["Attention Is All You Need", "BERT: Pre-training of Deep Bidirectional Transformers", "GPT-3: Language Models are Few-Shot Learners", "Diffusion Models Beat GANs on Image Synthesis"] )

style_input = st.selectbox( "Select Explanation Style", ["Beginner-Friendly", "Technical", "Code-Oriented", "Mathematical"] ) 

length_input = st.selectbox( "Select Explanation Length", ["Short (1-2 paragraphs)", "Medium (3-5 paragraphs)", "Long (detailed explanation)"] )

template = load_prompt('template.json')



if st.button('Summarize'):
    chain = template | model
    result = chain.invoke({
        'paper_input':paper_input,
        'style_input':style_input,
        'length_input':length_input
    })
    st.write(result.content)



####################
CHATBOT WITH CHAT HISTORY

-> toh chat history mein AI and Human mssg ko alag alag dic mein stored hoga
-> Langchain has 3 types of message s- System, AI and Human


from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI()

chat_history = [
    SystemMessage(content='You are a helpful AI assistant')
]

while True:
    user_input = input('You: ')
    chat_history.append(HumanMessage(content=user_input))
    if user_input == 'exit':
        break
    result = model.invoke(chat_history)
    chat_history.append(AIMessage(content=result.content))
    print("AI: ",result.content)

print(chat_history)



from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI()

messages=[
    SystemMessage(content='You are a helpful assistant'),
    HumanMessage(content='Tell me about LangChain')
]

result = model.invoke(messages)

messages.append(AIMessage(content=result.content))

print(messages)



-> Agents
LLM with reasoning capabilities and tools ka access






Terms
- prompt engineering
- RLHF
- RAG
