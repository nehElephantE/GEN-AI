-> RNN - type of neural network designed for sequential data (time-series, text, speech, etc.)
Unlike normal feedforward neural networks (which assume inputs are independent), RNNs remember past inputs using a hidden state, making them ideal for sequences.
Components:
Input layer (xₜ) → Sequence element at time step t (e.g., a word vector).
Hidden layer (hₜ) → Stores memory of past inputs.
Output layer (yₜ) → Prediction/output at each time step.
Weights:
Wₓh: Input → hidden
Whh: Hidden → hidden (recurrent connection)
Why: Hidden → output

ht​=tanh(Wxh​xt​+Whh​ht−1​+bh​) - Hidden state update
yt​=softmax(Why​ht​+by​) - Output prediction


NOTE : initially NLP and other sequential tasks were done using RNN (LSTM/GRU) but parallel processing was tough that why transformers came into picture


-> Vector Embeddings
way of representing data (like words, images, audio, or even entire documents) as a list of numbers (a vector).
Embeddings turn “things” into numbers while preserving their meaning.

A simple way would be one-hot encoding (assign each word a unique ID and represent it as a binary vector), 
but this fails to capture relationships (like king ≠ queen but they’re related).
Embeddings solve this by mapping similar things close together in vector space.

A good embedding has these qualities:
Semantic similarity → Similar items are close in vector space (cosine similarity or Euclidean distance).
“cat” and “dog” will be closer than “cat” and “banana.”
Context-awareness → Words or objects get meaning from context.
“bank” (river) vs. “bank” (finance).
Low-dimensional → Compressed representation compared to sparse vectors (like one-hot).
Example: One-hot encoding of 50,000 words = 50,000 dimensions. Embeddings can represent this in just 300 dimensions.



-> SoftMax
Softmax is a mathematical function that converts a vector of real numbers (logits) into a probability distribution.
It takes numbers that could be negative, positive, or large/small, and squashes them into values between 0 and 1, where all outputs add up to 1.

z=[z1,z2,...,zn].
Softmax(zi)=e^(zi)/∑ e^(zi)

z=[2.0,1.0,0.1]
ez=[e2.0,e1.0,e0.1]≈[7.39,2.72,1.10] --- 7.39+2.72+1.10=11.21 
[7.39/11.21,2.72/11.21,1.10/11.21]≈[0.66,0.24,0.10]
Class 1 → 66%........Class 2 → 24%.............Class 3 → 10%
This means the model is most confident about Class 1.

Why Exponential Function?
Exponentiation amplifies differences: A slightly higher logit leads to a much higher probability.

Key Uses of Softmax
Multi-class classification (last layer of a neural network).
Example: Cat, Dog, Bird → Softmax gives probability for each class.

Attention mechanism (in Transformers).
Used to decide how much attention a word should pay to another word.
The attention scores (dot products) are passed through Softmax → so they become weights that sum to 1.

Reinforcement learning policies.
Used to turn Q-values or logits into action probabilities.


Sigmoid: Special case of softmax with only 2 classes. (binary classification)
σ(x) = 1 / (1+ e^(-x))
Softmax: Generalizes to multiple classes.



->Attention Mechanism
way for the model to weight other tokens by relevance when processing a given token.
For each token 𝑖, the model asks: “Which other tokens 𝑗 matter most to me right now?”
The answers are numbers (weights) over all tokens; higher weight ⇒ more influence.

For a sequence of N tokens, the model builds three vectors per token (per head):
•	Query qi: what token i is looking for
•	Key kj: what token j offers (its address/descriptor)
•	Value vj: the actual information carried by token j













-> Transformer  - processes text in parallel instead of sequentially.
It takes an entire sentence at once.
Uses attention to figure out which words matter most to each other.
Components of Transformer Architecture
1. Input & Tokenization
Sentence → Tokens → Converted into vectors (embeddings).
“Playing football” → [“Play”, “ing”, “football”] → Numeric embeddings.

2. Positional Encoding
Since Transformers don’t read sequentially, they need extra info about word order.
Positional encoding adds a unique signal to each token embedding.
Formula uses sine & cosine functions with different frequencies.

3. Encoder-Decoder Structure
Encoder → Reads input text & creates hidden representations.
Decoder → Generates output sequence word by word (used in translation, summarization).

4. Multi-Head Self-Attention
heart of the Transformer.
Each token is turned into 3 vectors:
Query (Q) – "What am I looking for?"
Key (K) – "What information do I have?"
Value (V) – "The actual information I can provide."
The model calculates Attention Scores = Q • K (dot product).
High score = token is relevant.
Then weight the Values (V) accordingly.

Sentence: “The bank near the river flooded.”
Query(word=“bank”) compares with Key(“river”) → strong score → context = river bank.

Multi-head:
Multiple attention "heads" look at different aspects of relationships (syntax, semantics, long-distance dependencies).
Then combine their insights.

5. Feed-Forward Neural Network (FFN)
After attention, each token passes through a small feedforward network (same for all tokens).
This helps the model learn nonlinear patterns and richer representations.

6. Residual Connections & Layer Normalization
To prevent vanishing gradients and help training:
Residual connections (skip connections) → add input back to the output of a layer.
Layer normalization → keeps values stable.

7. Stacking Layers
Transformers stack many encoder/decoder layers (12, 24, 96+ depending on size).
Each layer refines the representation of tokens.
More layers = more complex reasoning ability.

8. Output Layer
Finally, a linear + softmax layer predicts the probability of the next token.


Variants of Transformers
Encoder-only → BERT (good for understanding tasks: classification, QA).
Decoder-only → GPT series (good for text generation, completion).
Encoder-Decoder → T5, original Transformer (good for translation, summarization).




-> LLM - type of artificial intelligence model trained on huge amounts of text data to understand and generate human-like language.
Core Components of LLMs
1. Architecture
Transformers introduced two key ideas:
Attention mechanism → lets the model “focus” on important words in a sentence, even if they’re far apart.
Self-attention → each word looks at all the others to understand context.

2. Training Data - massive text data

3. Parameters
knobs” or internal weights that the model learns during training.
The larger the model, the more parameters it has (e.g., GPT-3 → 175B, GPT-4 → trillions+).
Parameters store knowledge of grammar, facts, reasoning ability.

4. Tokenizer
Before feeding text to the model, words are split into tokens (smaller units).

5. Embedding Layer
Tokens are converted into vectors (numbers).
These vectors capture meaning — similar words have similar vectors.

6. Attention Mechanism
The heart of transformers.
Allows the model to weigh which words are important when predicting the next word.

7. Feedforward Layers
After attention, information passes through neural layers that transform data and learn higher-level features (like grammar rules, reasoning patterns).

8. Output Layer
The model outputs probabilities for the next token.
