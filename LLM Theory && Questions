-> RNN - type of neural network designed for sequential data (time-series, text, speech, etc.)
Unlike normal feedforward neural networks (which assume inputs are independent), RNNs remember past inputs using a hidden state, making them ideal for sequences.
Components:
Input layer (xâ‚œ) â†’ Sequence element at time step t (e.g., a word vector).
Hidden layer (hâ‚œ) â†’ Stores memory of past inputs.
Output layer (yâ‚œ) â†’ Prediction/output at each time step.
Weights:
Wâ‚“h: Input â†’ hidden
Whh: Hidden â†’ hidden (recurrent connection)
Why: Hidden â†’ output

htâ€‹=tanh(Wxhâ€‹xtâ€‹+Whhâ€‹htâˆ’1â€‹+bhâ€‹) - Hidden state update
ytâ€‹=softmax(Whyâ€‹htâ€‹+byâ€‹) - Output prediction


RNN Training Algorithm
1. Forward Pass
Process input sequence step by step, updating hidden states and computing outputs.

2. Loss Calculation
Compare predictions (yâ‚œ) to true labels â†’ compute loss (cross-entropy, MSE, etc.).

3. Backward Pass (BPTT â€“ Backpropagation Through Time)
Backpropagation is applied across time steps.
Errors flow backward through the unfolded sequence.

Problem:
Vanishing Gradient â†’ Gradients shrink â†’ canâ€™t learn long dependencies.
Exploding Gradient â†’ Gradients blow up â†’ unstable training.
Solutions: Gradient clipping, LSTMs/GRUs.


NOTE : initially NLP and other sequential tasks were done using RNN (LSTM/GRU) but parallel processing was tough that why transformers came into picture


-> Vector Embeddings
way of representing data (like words, images, audio, or even entire documents) as a list of numbers (a vector).
Embeddings turn â€œthingsâ€ into numbers while preserving their meaning.

A simple way would be one-hot encoding (assign each word a unique ID and represent it as a binary vector), 
but this fails to capture relationships (like king â‰  queen but theyâ€™re related).
Embeddings solve this by mapping similar things close together in vector space.

A good embedding has these qualities:
Semantic similarity â†’ Similar items are close in vector space (cosine similarity or Euclidean distance).
â€œcatâ€ and â€œdogâ€ will be closer than â€œcatâ€ and â€œbanana.â€
Context-awareness â†’ Words or objects get meaning from context.
â€œbankâ€ (river) vs. â€œbankâ€ (finance).
Low-dimensional â†’ Compressed representation compared to sparse vectors (like one-hot).
Example: One-hot encoding of 50,000 words = 50,000 dimensions. Embeddings can represent this in just 300 dimensions.



-> SoftMax
Softmax is a mathematical function that converts a vector of real numbers (logits) into a probability distribution.
It takes numbers that could be negative, positive, or large/small, and squashes them into values between 0 and 1, where all outputs add up to 1.

z=[z1,z2,...,zn].
Softmax(zi)=e^(zi)/âˆ‘ e^(zi)

z=[2.0,1.0,0.1]
ez=[e2.0,e1.0,e0.1]â‰ˆ[7.39,2.72,1.10] --- 7.39+2.72+1.10=11.21 
[7.39/11.21,2.72/11.21,1.10/11.21]â‰ˆ[0.66,0.24,0.10]
Class 1 â†’ 66%........Class 2 â†’ 24%.............Class 3 â†’ 10%
This means the model is most confident about Class 1.

Why Exponential Function?
Exponentiation amplifies differences: A slightly higher logit leads to a much higher probability.

Key Uses of Softmax
Multi-class classification (last layer of a neural network).
Example: Cat, Dog, Bird â†’ Softmax gives probability for each class.

Attention mechanism (in Transformers).
Used to decide how much attention a word should pay to another word.
The attention scores (dot products) are passed through Softmax â†’ so they become weights that sum to 1.

Reinforcement learning policies.
Used to turn Q-values or logits into action probabilities.


Sigmoid: Special case of softmax with only 2 classes. (binary classification)
Ïƒ(x) = 1 / (1+ e^(-x))
Softmax: Generalizes to multiple classes.



->Attention Mechanism
way for the model to weight other tokens by relevance when processing a given token.
For each token ð‘–, the model asks: â€œWhich other tokens ð‘— matter most to me right now?â€
The answers are numbers (weights) over all tokens; higher weight â‡’ more influence.

For a sequence of N tokens, the model builds three vectors per token (per head):
â€¢	Query qi: what token i is looking for
â€¢	Key kj: what token j offers (its address/descriptor)
â€¢	Value vj: the actual information carried by token j













-> Transformer  - processes text in parallel instead of sequentially.
It takes an entire sentence at once.
Uses attention to figure out which words matter most to each other.
Components of Transformer Architecture
1. Input & Tokenization
Sentence â†’ Tokens â†’ Converted into vectors (embeddings).
â€œPlaying footballâ€ â†’ [â€œPlayâ€, â€œingâ€, â€œfootballâ€] â†’ Numeric embeddings.

2. Positional Encoding
Since Transformers donâ€™t read sequentially, they need extra info about word order.
Positional encoding adds a unique signal to each token embedding.uu
Formula uses sine & cosine functions with different frequencies.

3. Encoder-Decoder Structure
Encoder â†’ Reads input text & creates hidden representations.
Decoder â†’ Generates output sequence word by word (used in translation, summarization).

4. Multi-Head Self-Attention
heart of the Transformer.
Each token is turned into 3 vectors:
Query (Q) â€“ "What am I looking for?"
Key (K) â€“ "What information do I have?"
Value (V) â€“ "The actual information I can provide."
The model calculates Attention Scores = Q â€¢ K (dot product).
High score = token is relevant.
Then weight the Values (V) accordingly.

Sentence: â€œThe bank near the river flooded.â€
Query(word=â€œbankâ€) compares with Key(â€œriverâ€) â†’ strong score â†’ context = river bank.

Multi-head:
Multiple attention "heads" look at different aspects of relationships (syntax, semantics, long-distance dependencies).
Then combine their insights.

5. Feed-Forward Neural Network (FFN)
After attention, each token passes through a small feedforward network (same for all tokens).
This helps the model learn nonlinear patterns and richer representations.

6. Residual Connections & Layer Normalization
To prevent vanishing gradients and help training:
Residual connections (skip connections) â†’ add input back to the output of a layer.
Layer normalization â†’ keeps values stable.

7. Stacking Layers
Transformers stack many encoder/decoder layers (12, 24, 96+ depending on size).
Each layer refines the representation of tokens.
More layers = more complex reasoning ability.

8. Output Layer
Finally, a linear + softmax layer predicts the probability of the next token.


Variants of Transformers
Encoder-only â†’ BERT (good for understanding tasks: classification, QA).
Decoder-only â†’ GPT series (good for text generation, completion).
Encoder-Decoder â†’ T5, original Transformer (good for translation, summarization).




-> LLM - type of artificial intelligence model trained on huge amounts of text data to understand and generate human-like language.
Core Components of LLMs
1. Architecture
Transformers introduced two key ideas:
Attention mechanism â†’ lets the model â€œfocusâ€ on important words in a sentence, even if theyâ€™re far apart.
Self-attention â†’ each word looks at all the others to understand context.

2. Training Data - massive text data

3. Parameters
knobsâ€ or internal weights that the model learns during training.
The larger the model, the more parameters it has (e.g., GPT-3 â†’ 175B, GPT-4 â†’ trillions+).
Parameters store knowledge of grammar, facts, reasoning ability.

4. Tokenizer
Before feeding text to the model, words are split into tokens (smaller units).

5. Embedding Layer
Tokens are converted into vectors (numbers).
These vectors capture meaning â€” similar words have similar vectors.

6. Attention Mechanism
The heart of transformers.
Allows the model to weigh which words are important when predicting the next word.

7. Feedforward Layers
After attention, information passes through neural layers that transform data and learn higher-level features (like grammar rules, reasoning patterns).

8. Output Layer
The model outputs probabilities for the next token.


Temperature: A setting that controls the randomness of the output.
Low Temperature (e.g., 0.2): More deterministic and focused. It will likely choose the most probable next word every time. Good for factual, consistent responses.
High Temperature (e.g., 0.8): More creative and random. It's more likely to choose less probable words. Good for storytelling and generating ideas.

Context Window (or Context Length): The maximum amount of text (tokens) the model can consider at one time. This includes your prompt and its response. 
A larger context window (e.g., 128K tokens) allows it to "remember" more within a single conversation or analyze very long documents.



-> 
Top-k sampling restricts the modelâ€™s choices to the top k most
 probable tokens at each step, introducing controlled randomness.

Nucleus sampling, or top-p sampling, takes a
 more dynamic approach by selecting tokens whose cumulative
 probability exceeds a threshold p (e.g., 0.9). 





->  LoRA and QLoRA
these are 2 fine-tuning techniques
LoRA (Low-Rank Adaptation)
QLoRA (Quantized LoRA)

Low-Rank Update Matrices ....When we train large models (like LLMs), most of the parameters are in weight matrices (big matrices of numbers). 
For example, in a Transformer layer, we often see weight matrices of size like 4096 Ã— 4096.
Updating these full matrices during training is computationally very expensive and requires huge memory.
The trick is: instead of updating the entire large matrix, we can approximate the update using low-rank matrices.

The rank of a matrix = the number of linearly independent rows/columns.....A low-rank matrix captures the â€œcoreâ€ structure of a matrix but with fewer dimensions.


LoRA ....Instead of tuning every weight and parameter (which is slow and expensive), LoRA "freezes" the main model and adds tiny, 
lightweight blocks called low-rank matrices. These matrices represent the essential changes needed for the new task and have far fewer values
than original weightsâ€”making them much faster and cheaper to train.

QLoRA...The rank of a matrix = the number of linearly independent rows/columns.....A low-rank matrix captures the â€œcoreâ€ structure of a matrix but with 
fewer dimensions.



->
Autoregressive model......generate text by predicting the next word (or token) in a sequence using only the words that have come before itâ€”one at a time, 
from left to right.
use a causal or masked self-attention mechanism....is a technical way of saying that when the model is calculating the meaning of a word, 
it is only allowed to look at the words that came before it. It must ignore (mask) all future word
Produces highly coherent and context-aware text; widely used in chatbots, translators, and AI writing tools
GPT((Generative Pre-trained Transformer) and RNNs

Masked Models.....predict missing or masked words in a sentence by using context from both sidesâ€”before and after the blank.
BERT (Bidirectional Encoder Representations from Transformers)
 use standard self-attention....Every word can attend to every other word in the sentence, in all directions.
Because it uses information before and after masked words, it learns richer context and is excellent for understanding, filling gaps, 
and finding relationships in text.
itâ€™s best for classification, searching, and tasks like question answering, not for generating text from scratch.
