-> RNN - type of neural network designed for sequential data (time-series, text, speech, etc.)
Unlike normal feedforward neural networks (which assume inputs are independent), RNNs remember past inputs using a hidden state, making them ideal for sequences.
Components:
Input layer (xâ‚œ) â†’ Sequence element at time step t (e.g., a word vector).
Hidden layer (hâ‚œ) â†’ Stores memory of past inputs.
Output layer (yâ‚œ) â†’ Prediction/output at each time step.
Weights:
Wâ‚“h: Input â†’ hidden
Whh: Hidden â†’ hidden (recurrent connection)
Why: Hidden â†’ output

htâ€‹=tanh(Wxhâ€‹xtâ€‹+Whhâ€‹htâˆ’1â€‹+bhâ€‹) - Hidden state update
ytâ€‹=softmax(Whyâ€‹htâ€‹+byâ€‹) - Output prediction


NOTE : initially NLP and other sequential tasks were done using RNN (LSTM/GRU) but parallel processing was tough that why transformers came into picture


-Attention Mechanism
way for the model to weight other tokens by relevance when processing a given token.
For each token ğ‘–, the model asks: â€œWhich other tokens ğ‘— matter most to me right now?â€
The answers are numbers (weights) over all tokens; higher weight â‡’ more influence.












-> Transformer  - processes text in parallel instead of sequentially.
It takes an entire sentence at once.
Uses attention to figure out which words matter most to each other.
Components of Transformer Architecture
1. Input & Tokenization
Sentence â†’ Tokens â†’ Converted into vectors (embeddings).
â€œPlaying footballâ€ â†’ [â€œPlayâ€, â€œingâ€, â€œfootballâ€] â†’ Numeric embeddings.

2. Positional Encoding
Since Transformers donâ€™t read sequentially, they need extra info about word order.
Positional encoding adds a unique signal to each token embedding.
Formula uses sine & cosine functions with different frequencies.

3. Encoder-Decoder Structure
Encoder â†’ Reads input text & creates hidden representations.
Decoder â†’ Generates output sequence word by word (used in translation, summarization).

4. Multi-Head Self-Attention
heart of the Transformer.
Each token is turned into 3 vectors:
Query (Q) â€“ "What am I looking for?"
Key (K) â€“ "What information do I have?"
Value (V) â€“ "The actual information I can provide."
The model calculates Attention Scores = Q â€¢ K (dot product).
High score = token is relevant.
Then weight the Values (V) accordingly.

Sentence: â€œThe bank near the river flooded.â€
Query(word=â€œbankâ€) compares with Key(â€œriverâ€) â†’ strong score â†’ context = river bank.

Multi-head:
Multiple attention "heads" look at different aspects of relationships (syntax, semantics, long-distance dependencies).
Then combine their insights.

5. Feed-Forward Neural Network (FFN)
After attention, each token passes through a small feedforward network (same for all tokens).
This helps the model learn nonlinear patterns and richer representations.

6. Residual Connections & Layer Normalization
To prevent vanishing gradients and help training:
Residual connections (skip connections) â†’ add input back to the output of a layer.
Layer normalization â†’ keeps values stable.

7. Stacking Layers
Transformers stack many encoder/decoder layers (12, 24, 96+ depending on size).
Each layer refines the representation of tokens.
More layers = more complex reasoning ability.

8. Output Layer
Finally, a linear + softmax layer predicts the probability of the next token.


Variants of Transformers
Encoder-only â†’ BERT (good for understanding tasks: classification, QA).
Decoder-only â†’ GPT series (good for text generation, completion).
Encoder-Decoder â†’ T5, original Transformer (good for translation, summarization).




-> LLM - type of artificial intelligence model trained on huge amounts of text data to understand and generate human-like language.
Core Components of LLMs
1. Architecture
Transformers introduced two key ideas:
Attention mechanism â†’ lets the model â€œfocusâ€ on important words in a sentence, even if theyâ€™re far apart.
Self-attention â†’ each word looks at all the others to understand context.

2. Training Data - massive text data

3. Parameters
knobsâ€ or internal weights that the model learns during training.
The larger the model, the more parameters it has (e.g., GPT-3 â†’ 175B, GPT-4 â†’ trillions+).
Parameters store knowledge of grammar, facts, reasoning ability.

4. Tokenizer
Before feeding text to the model, words are split into tokens (smaller units).

5. Embedding Layer
Tokens are converted into vectors (numbers).
These vectors capture meaning â€” similar words have similar vectors.

6. Attention Mechanism
The heart of transformers.
Allows the model to weigh which words are important when predicting the next word.

7. Feedforward Layers
After attention, information passes through neural layers that transform data and learn higher-level features (like grammar rules, reasoning patterns).

8. Output Layer
The model outputs probabilities for the next token.
